import matplotlib.pyplot as plt
import pandas as pd
import pickle
from sklearn.metrics import precision_score, recall_score, f1_score,\
    accuracy_score, classification_report, confusion_matrix,\
    ConfusionMatrixDisplay
from pathlib import Path

def check_directory_exists(dir_path):
    """
    Checks if the directory exists, if it does not create it.

    Parameters
    ----------
    dir_path : str
        The path of the directory to check or create.

    Returns
    -------
    pathlib.Path
        A Path object representing the verified or newly created directory.
    """
    path = Path(dir_path)
    if not path.exists():
        path.mkdir(parents=True, exist_ok=True)
    
    return path


def plot_save_confusion_matrix(y_obs, y_pred, model, plots_to):
    """
    Creates a confusion matrix plot from observed and predicted values, 
    then saves the plot to the specified directory.

    Parameters
    ----------
    y_obs : pd.Series or np.ndarray
        The true labels (observed values) of the target variable.
    y_pred : pd.Series or np.ndarray
        The predicted labels generated by the model.
    model : object
        A trained model object.
    plots_to : pathlib.Path
        The directory path where the confusion matrix plot will be saved.

    Returns
    -------
    None
        This function saves the plot to the directory without returning any value.
    
    Notes
    -----
    - The confusion matrix is displayed with a "Blues" colormap for visual clarity.
    - The saved plot is named "confusion_matrix.png".
    - The model object contains a `classes_` attribute, associated with scikit-learn model.
    """

    # Check if the model has the `classes_` attribute
    if not hasattr(model, 'classes_'):
        raise AttributeError(f"The model object is missing the 'classes_' attribute.")

    cm = confusion_matrix(y_obs, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    disp.plot(cmap="Blues")
    plt.title("Confusion Matrix")
    plt.tight_layout()
    conf_matrix_save_path = plots_to / "confusion_matrix.png"
    disp.figure_.savefig(conf_matrix_save_path)
    print(f"Confusion matrix saved in the directory: \033[1m{conf_matrix_save_path}\033[0m\n")


def evaluate_model(y_obs, y_pred, results_to):
    """
    Evaluates the performance of a classification model using various metrics and 
    saves the evaluation results to the specified directory.

    Parameters
    ----------
    y_obs : pd.Series or np.ndarray
        The true labels (observed values) of the target variable.
    y_pred : pd.Series or np.ndarray
        The predicted labels generated by the model.
    results_to : pathlib.Path
        The directory path where the evaluation metrics and classification report will be saved.

    Returns
    -------
    None
        This function saves the plot to the directory without returning any value.
    
    Notes
    -----
    - Saves a CSV file named `test_scores.csv` containing the performance metrics.
    - Saves another CSV file named `classification_report.csv` containing the classification report.
    - Metrics Calculated
        - Accuracy : the proportion of correct predictions to the total number of predictions
        - Recall : the ability of the model to correctly identify positive cases
        - Precision : the proportion of true positive predictions to the positive predictions
        - F-1 Score : the harmonic mean of precision and recall
    """
    # calculates model performance based on metrics
    print("evaluate_model called.")
    print(f"y_test: {y_obs}")
    print(f"y_test_pred: {y_pred}")
    print(f"results_to: {results_to}")

    scoring_metrics = pd.DataFrame({
        "Accuracy": [accuracy_score(y_obs, y_pred)],
        "Recall": [recall_score(y_obs, y_pred, pos_label = 'satisfied')],
        "Precision": [precision_score(y_obs, y_pred, pos_label = 'satisfied')],
        "F1-Score": [f1_score(y_obs, y_pred, pos_label = 'satisfied')]
    })
    test_scores_save_path = results_to / "test_scores.csv"
    scoring_metrics.to_csv(test_scores_save_path, index=False)
    print(f"Test scores saved in the directory: \033[1m{test_scores_save_path}\033[0m\n")

    # Create a classification report and save it
    class_report = pd.DataFrame(classification_report(y_obs, y_pred, output_dict=True))
    class_report_save_path = results_to / "classification_report.csv"
    class_report.to_csv(class_report_save_path, index=False)
    print(f"Classification report saved in the directory: \033[1m{class_report_save_path}\033[0m\n")    


def main(pipeline, test_path, results_to, plots_to):
    """
    Main function to evaluate a trained model on test data, save evaluation metrics,
    and generate plots.

    This function reads the test dataset, loads a pre-trained model pipeline, evaluates 
    the model's performance on the test dataset, and saves the results (including metrics 
    and plots) to the specified directories.
    
    Parameters
    ----------
    pipeline : str
        File path to the pickled model pipeline to be evaluated.
    test_path : str
        File path to the testing dataset in CSV format.
    results_to : str
        Directory path where evaluation metrics and classification reports will be saved as CSV files.
    plots_to : str
        Directory path where evaluation plots will be saved.

    Returns
    -------
    None
        This function saves the plot to the directory without returning any value.
    """

    results_to = check_directory_exists(results_to)
    plots_to = check_directory_exists(plots_to)

    # Prepare the test set
    test_data = pd.read_csv(test_path)
    X_test = test_data.drop(columns=['satisfaction'])
    y_test = test_data['satisfaction'].values.ravel()

    # Predict and evaluate on the test set
    final_model = pickle.load(open(pipeline, "rb"))
    y_test_pred = final_model.predict(X_test)
    evaluate_model(y_test, y_test_pred, results_to)
    plot_save_confusion_matrix(y_test, y_test_pred, final_model, plots_to)


if __name__ == "__main__":
    try:
        main()
        print("Congratulations! Model Evaluation Done!")
    except Exception as e:
        print(f"The following error occurred: {e}")